
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/august_tests.ipynb
from pyforest import *
from collections import Counter

import boto3
from botocore import UNSIGNED
from botocore.config import Config
import botocore

from PIL import Image

from wordcloud import WordCloud

# for DL
from multiprocessing import Pool, Manager
import functools
import logging

# define s3 object
def get_s3():
    s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))
    bucket = "open-images-dataset"

    return s3, bucket


# some data manipulations

def get_data_sets():
    classes = pd.read_csv(f'{data_root}/class-descriptions-boxable.csv',header=None)

    df = pd.read_csv(f'{data_root}/test-annotations-bbox.csv')

    c = Counter(df.LabelName)
    class_count = pd.DataFrame.from_dict(dict(c),orient='index').reset_index()

    class_name_count=pd.merge(classes,class_count,left_on=0,right_on='index')[['index',1,'0_y']]
    class_name_count.columns = ['id','name','count']

    class_dict = {i[1]['name']:i[1]['count'] for i in class_name_count[['name','count']].iterrows()}

    id2name = {i[1].id:i[1]['name'] for i in class_name_count.iterrows()}
    return df,class_name_count,  id2name


# fast DL with pool

n_work=10

def download(bucket, root, retry, counter, lock, path):
    i = 0
    src = path
    dest = f"{root}/{path}"
    while i < retry:
        try:

            if not os.path.exists(dest):
                s3.download_file(bucket, src, dest)

            else:
                logging.info(f"{dest} already exists.")
            with lock:
                counter.value += 1
                if counter.value % 100 == 0:
                    logging.warning(f"Downloaded {counter.value} images.")
            return
        except botocore.exceptions.ClientError as e:
            if e.response['Error']['Code'] == "404":
                logging.warning(f"The file s3://{bucket}/{src} does not exist.")
                return
            i += 1
            logging.warning(f"Sleep {i} and try again.")
            time.sleep(i)

def batch_download(bucket, file_paths, root, num_workers=10, retry=10):
    with Pool(num_workers) as p:
        m = Manager()
        counter = m.Value('i', 0)
        lock = m.Lock()
        download_ = functools.partial(download, bucket, root, 3, counter, lock)
        p.map(download_, file_paths)

def prepare_dirs():
    pass

def dl_classes(classes):
    try:
        ids = [class_name_count[class_name_count.name == c].id.values[0] for c in classes]
    except:
        print('class does not exist')
    df1 = df[df.LabelName.isin(ids)]
    print(len(df1))
    imgs = df1.drop_duplicates('ImageID')
    for j in range(len(imgs)//n_work):
        # {id2name[i[1].LabelName]} this should be the folder
        image_files = [f'test/{i[1].ImageID}.jpg' for i in imgs[(n_work)*j:n_work*(j+1)].iterrows()]
        batch_download(bucket, image_files, data_root, n_work, False)

if __name__ == '__main__':
    data_root = '/home/gidi/data/open images/'

    s3, bucket = get_s3()

    classes = ['Orange','Banana','Apple']

    df ,class_name_count, id2name =get_data_sets()

    # currently prepare dirs manually
    dl_classes(classes)